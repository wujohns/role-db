# 维基百科数据本地化
这里主要对维基百科的数据本地化处理方案做整理，以便于作为后续的工程支撑

## wiki 数据下载
wiki 的数据可以从其官方下载，这里选择的是中文最新数据:  
1. 下载连接: https://dumps.wikimedia.org/zhwiki/latest/  
1. 选择 zhwiki-latest-pages-articles.xml.bz2 进行下载(文章信息，截止到23/6/8为止，大小为 2.5G)  

## wiki 数据解析
1. 使用 gensim.corpora.wikicorpus 来对 wiki 数据做解析  
方案简化代码如下:  
```py
import os
from gensim.corpora.wikicorpus import extract_pages,filter_wiki
import bz2file
import re
import opencc
from tqdm import tqdm
import codecs

converter = opencc.OpenCC('t2s.json')
def wiki_replace(d):
  # 文本格式化解析
  s = d[1]
  s = re.sub(':*{\|[\s\S]*?\|}', '', s)
  s = re.sub('<gallery>[\s\S]*?</gallery>', '', s)
  s = re.sub('(.){{([^{}\n]*?\|[^{}\n]*?)}}', '\\1[[\\2]]', s)
  s = filter_wiki(s)
  s = re.sub('\* *\n|\'{2,}', '', s)
  s = re.sub('\n+', '\n', s)
  s = re.sub('\n[:;]|\n +', '\n', s)
  s = re.sub('\n==', '\n\n==', s)

  # 标题
  title = d[0]

  # 摘要抽取
  abstract_match = re.search(title + '[\s\S]*?\n==', s)
  abstract = ''
  if (abstract_match):
    abstract = abstract_match.group()
    abstract = re.sub('\n==', '', abstract)
    abstract = converter.convert(abstract).strip()

  # 标题部分
  s = u'【' + d[0] + u'】\n' + s
  return converter.convert(s).strip()

# 绝对路径获取方法
curPath = os.path.dirname(os.path.abspath(__file__))
def getAbsPath (relativePath):
  joinPath = os.path.join(curPath, relativePath)
  return os.path.normpath(
    os.path.abspath(joinPath)
  )

# wiki xml 压缩文件路径
wiki_path = getAbsPath('./zhwiki-latest-pages-articles.xml.bz2')
wiki = extract_pages(bz2file.open(wiki_path))

i = 0
f = codecs.open('wiki.txt', 'w', encoding='utf-8')
w = tqdm(wiki, desc=u'已获取0篇文章')
for d in w:
  # 这里的逻辑有待斟酌，其把标题为字母开头的都排除了
  if not re.findall('^[a-zA-Z]+:', d[0]) and d[0] and not re.findall(u'^#', d[1]):
    s = wiki_replace(d)
    f.write(s+'\n\n\n')
    i += 1
    if i % 100 == 0:
      w.set_description(u'已获取%s篇文章'%i)

f.close()
```

## 信息转储与查询方案
采用向量数据库存储方案
TODO 一些细节需要思考
