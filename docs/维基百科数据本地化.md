# 维基百科数据本地化
这里主要对维基百科的数据本地化处理方案做整理，以便于作为后续的工程支撑

## wiki 数据下载
wiki 的数据可以从其官方下载，这里选择的是中文最新数据:  
1. 下载连接: https://dumps.wikimedia.org/zhwiki/latest/  
1. 选择 zhwiki-latest-pages-articles.xml.bz2 进行下载(文章信息，截止到23/6/8为止，大小为 2.5G)  
1. 下载的文件放在本工程的 wiki-dump 目录中(没有则手动创建一个)  

## wiki 数据解析与存储
这里将数据格式化后存入到 mongo 中
1. 参考 [wiki-mongo/mongo.py](/wiki-mongo/mongo.py) 中的相关实现(预计耗时6小时)  
1. 也可以参考 [wiki-mongo/quick_build.py](/wiki-mongo/quick_build.py)，这个是并行计算版本，速度更快(预计耗时20分钟)  

## wiki 数据向量化
TODO 对 faiss 做系统学习，后续使用 faiss 来支撑该部分的业务
